<!DOCTYPE html>
<html lang="en-US">
<head>
<!-- Meta Tags -->
    <meta charset="UTF-8">
    <meta property="og:title" content="Remember: A Link to the Past - Murilo Krominski."> <!-- Title for social media previews --> <!-- üë®üèª‚Äçüíª -->
    <meta property="og:description" content="Discover the work and projects of Murilo Krominski."> <!-- Description for social media previews --> <!-- üë®üèª‚Äçüíª -->
    <meta property="og:image" content="https://murilokrominski.github.io/Remember-A-Link-to-the-Past/Remember-A-Link-to-the-Past.png"> <!-- Image URL for social media previews --> <!-- üë®üèª‚Äçüíª -->
    <meta property="og:url" content="https://murilokrominski.github.io/Remember-A-Link-to-the-Past"> <!-- Page URL for social media previews --> <!-- üë®üèª‚Äçüíª -->
    <meta name="description" content="Portfolio of Murilo Krominski."> <!-- Short page description for search engines --> <!-- üë®üèª‚Äçüíª -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="msapplication-TileColor" content="#0038A8">
    <meta name="theme-color" content="#FFFFFF">

<!-- https://murilokrominski.github.io/Remember-A-Link-to-the-Past - Remember: A Link to the Past -->
<title>Remember: A Link to the Past - Murilo Krominski</title>

<!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="https://murilokrominski.github.io/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://murilokrominski.github.io/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://murilokrominski.github.io/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="https://murilokrominski.github.io/favicon_io/site.webmanifest">
    <link rel="mask-icon" href="https://murilokrominski.github.io/favicon_io/safari-pinned-tab.svg" color="#0038A8">
<!-- External Stylesheets -->
    <link rel="stylesheet" href="style.css">
</head>
<!-- Body -->
<body>
<!-- Iframe Sponsor -->
    <iframe src="https://github.com/sponsors/MuriloKrominski/button" title="Sponsor MuriloKrominski" height="32" width=100% style="border: 0; border-radius: 6px;"></iframe>
    <br>
    <br>
<!-- Language selector with flags -->
<div class="language-selector">
    <center>
    <!-- Flag for English (US) -->
    <a href="" onclick="changeLanguage('en')">
    <img alt="US flag for language switch" src="https://upload.wikimedia.org/wikipedia/commons/a/a4/Flag_of_the_United_States.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Portuguese (Brazil) -->
    <a href="" onclick="changeLanguage('pt')">
    <img alt="Brazilian flag for language switch" src="https://upload.wikimedia.org/wikipedia/en/0/05/Flag_of_Brazil.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Italian -->
    <a href="" onclick="changeLanguage('it')">
    <img alt="Italian flag for language switch" src="https://upload.wikimedia.org/wikipedia/en/0/03/Flag_of_Italy.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Spanish (Argentina) -->
    <a href="" onclick="changeLanguage('es')">
    <img alt="Argentinian flag for language switch" src="https://upload.wikimedia.org/wikipedia/commons/1/1a/Flag_of_Argentina.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Chinese (Mandarin) -->
    <a href="" onclick="changeLanguage('zh-CN')">
    <img alt="Chinese flag for language switch" src="https://upload.wikimedia.org/wikipedia/commons/f/fa/Flag_of_the_People%27s_Republic_of_China.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Russian -->
    <a href="" onclick="changeLanguage('ru')">
    <img alt="Russian flag for language switch" src="https://upload.wikimedia.org/wikipedia/en/f/f3/Flag_of_Russia.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Hebrew -->
    <a href="" onclick="changeLanguage('iw')">
    <img alt="Israeli flag for language switch" src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Flag_of_Israel.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Korean -->
    <a href="" onclick="changeLanguage('ko')">
    <img alt="Korean flag for language switch" src="https://upload.wikimedia.org/wikipedia/commons/0/09/Flag_of_South_Korea.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Japanese -->
    <a href="" onclick="changeLanguage('ja')">
    <img alt="Japanese flag for language switch" src="https://upload.wikimedia.org/wikipedia/en/9/9e/Flag_of_Japan.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for German -->
    <a href="" onclick="changeLanguage('de')">
    <img alt="German flag for language switch" src="https://upload.wikimedia.org/wikipedia/en/b/ba/Flag_of_Germany.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Hindi -->
    <a href="" onclick="changeLanguage('hi')">
    <img alt="Indian flag for language switch" src="https://upload.wikimedia.org/wikipedia/en/4/41/Flag_of_India.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for French -->
    <a href="" onclick="changeLanguage('fr')">
    <img alt="French flag for language switch" src="https://upload.wikimedia.org/wikipedia/en/c/c3/Flag_of_France.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    <!-- Flag for Arabic -->
    <a href="" onclick="changeLanguage('ar')">
    <img alt="Egyptian flag for language switch" src="https://upload.wikimedia.org/wikipedia/commons/f/fe/Flag_of_Egypt.svg" style="border-radius: 5px;" width="40" height="auto"/>
    </a>
    </center>
</div>
<!-- Autor -->
    <hr>
    <center>
    <a href="https://murilokrominski.github.io/autor.htm">
    <img src="https://murilokrominski.github.io/media/avatar.jpeg" alt="autor" style="max-width: 160px; max-height: 160px; width: auto; height: auto;">
    </a>
    <br>
    By <a href="https://murilokrominski.github.io/autor.htm">Murilo Krominski</a>
    <br>
    <a href="https://murilokrominski.github.io/autor.htm">
    <img src="https://img.shields.io/badge/https://murilokrominski.github.io/autor.htm-blue.svg" alt="Autor">
    </a>
    <a href="https://murilokrominski.github.io/">
    <img src="https://img.shields.io/badge/Projects - Repository Œ≤ (PUBLIC)-orange.svg" alt="Repository Œ≤ (PUBLIC)">
    </a>
    </center>
    <hr>




<!-- Project Section: Start -->
<div class="container">
    <center>
        <!-- Project Title and Author -->
        <h1 id="remember-a-link-to-the-past-v1-0-beta">
            Remember: A Link to the Past <code>v1.0-beta</code>
        </h1>
    </center>

    <!-- Project Description -->
    <table align="center" style="max-width: 640px; width: 100%; height: auto;" border="0" cellpadding="0" cellspacing="0">
        <tr>
        <td>
        <p style="text-align: justify;">
            <strong>Remember: A Link to the Past</strong>, allows companies to "go back in time" and review past activities. It automatically captures screenshots at set intervals, processes images with AI to identify open applications, transcribe text, and generate detailed metadata.
        </p>
        <p style="text-align: justify;">
            The information is stored in a vectorized database, enabling fast and accurate searches. With an intuitive interface, the system supports semantic searches to locate specific events, documents, or details from past moments.
        </p>
        <p style="text-align: justify;">
            Ideal for companies of all sizes, Remember centralizes all essential information, boosting efficiency and ensuring security with local storage. It's a practical, cost-effective solution to protect data and optimize productivity.
        </p>
        </td>
        </tr>
        </table>
        </center>
    </div>

        <!-- Process -->
        <center>
            <img src="https://raw.githubusercontent.com/MuriloKrominski/Remember-A-Link-to-the-Past/refs/heads/main/process.svg" alt="process" style="max-width: 640px; width: 100%; height: auto;">
            </center>

    <!-- Project Sections -->
    <h2 id="what-is-remember-a-link-to-the-past">What is "Remember: A Link to the Past"?</h2>
    <p style="text-align: justify;">
        <strong>Remember: A Link to the Past</strong> is an open-source solution developed for companies and teams looking to optimize information management and retrieval. It periodically captures the user's screen, processes the images with artificial intelligence, and stores the data in a database. With an intuitive interface, it allows for quick and efficient searches of any past moments.
    </p>
    <p style="text-align: justify;">
        The advanced technology of Remember centralizes and organizes all essential information, ensuring quick and accurate access to documents, data, and insights, transforming the team's workflow.
    </p>

    <!-- Key Benefits -->
    <h2 id="key-benefits">üöÄ Key Benefits</h2>
    <ul>
        <li><strong>Intelligent Search:</strong> Quickly recover documents and records with the help of AI.</li>
        <li><strong>Centralized Organization:</strong> Store and access information securely and conveniently.</li>
        <li><strong>Real-Time Collaboration:</strong> Agile sharing and integration with other tools.</li>
        <li><strong>Privacy and Security:</strong> Full control over data with local storage options.</li>
        <li><strong>Productivity:</strong> Increase efficiency with fast access to information.</li>
        <li><strong>Cost-Effective:</strong> Robust solution without the need for advanced hardware.</li>
    </ul>

        <!-- Process -->
        <center>
        <img src="https://raw.githubusercontent.com/MuriloKrominski/Remember-A-Link-to-the-Past/refs/heads/main/process2.svg" alt="process" style="max-width: 640px; width: 100%; height: auto;">
        </center>

    <!-- Who is it Ideal for -->
    <h2 id="who-is-it-ideal-for">üéØ Who is it Ideal for</h2>
    <ul>
        <li><strong>Companies of all sizes</strong> that need to optimize data organization and retrieval.</li>
        <li><strong>IT professionals and managers</strong> looking to centralize team knowledge.</li>
        <li><strong>Marketing, sales, and operations teams</strong> that need agility to access information.</li>
    </ul>

    <!-- Workflow and Features -->
    <h2 id="workflow-and-features">üîÑ Workflow and Features</h2>
    <ol>
        <li><strong>Automated Screenshot Capture with Local Storage:</strong> Captures screenshots at defined time intervals and stores them locally.</li>
        <li><strong>AI Image Processing Pipeline:</strong> Captured images are processed to identify open applications, transcribe texts, and generate descriptions and other metadata.</li>
        <li><strong>Vectorized Database Storage:</strong> Extracted descriptions and metadata are saved and vectorized in a database, allowing fast and efficient searches in the future.</li>
        <li><strong>Search for Past Events:</strong> Through a simple interface, users can search previous moments using an efficient semantic search to find specific events.</li>
        <li><strong>Voice-Driven Mobile App:</strong> Enables voice-based searches for past events with NLP intent recognition and custom filters, providing relevant results through a streamlined mobile interface. [Prototype]</li>
    </ol>
    <!-- Important Considerations -->
    <h2>üö® Important Considerations</h2>
    <p>I, Murilo Krominski, see this project as a promising prototype with great potential for development and numerous possibilities for improvement. I recognize that its current form has room for enhancement and that the proposed approach may require specific adjustments for certain purposes. Nevertheless, the project is designed to be flexible and adaptable to various needs.</p>
    
    <p>I invite other developers to join me on this journey, contributing code improvements and implementations that can further strengthen and expand this project. As an open-source initiative, it has the potential to benefit countless people, whether through the complete project or the modular implementation of its individual components.</p>
    
    <p>I have several ideas for improvements and new features, but these require dedicated time and resources. Therefore, I also invite those who see value in this project to consider contributing financially to enable its development. Financial support will allow the project to grow with greater speed and quality, always prioritizing data security and efficiency.</p>
    
    <p>I recognize that some parts are still in development, as I believe it is essential to conduct rigorous testing before releasing any finalized version for download. I prefer to ensure that each aspect functions reliably before providing a complete version that meets the highest standards of quality and usability.</p>
<!-- Step-by-Step Implementation -->
<h2 id="step-by-step-implementation">üîß Step-by-Step Implementation</h2>
<p>Here is an overview of the implementation steps and code suggestions:</p>



<hr>
<!-- Project 1: Automated Screenshot Capture with Local Storage -->
    <h1>Project 1: Automated Screenshot Capture with Local Storage</h1>
    <h2>Objective</h2>
    <p>Develop a script that automatically captures screenshots at predefined intervals and stores these images locally. 
    The script must be robust, allowing for future expansions such as AI processing integration and database storage.</p>

    <h2>Project Structure and Rationale</h2>

    <h3>Tools and Libraries Selected</h3>
    <ul>
        <li><strong>Python</strong>: Python is chosen as the primary language for its versatility and wide usage in automation and data science. It offers excellent support for image processing, scheduling, and AI integration‚Äîessential for future project enhancements.</li>
        <li><strong>Pillow Library (PIL)</strong>: We selected Pillow for image capture and manipulation because it supports multiple image formats, integrates seamlessly with Python, and is well-documented.</li>
        <li><strong>schedule Library</strong>: This lightweight, straightforward library enables scheduling tasks at regular intervals, simplifying interval management without complex thread handling.</li>
    </ul>

    <h2>Code Walkthrough and Explanations</h2>

    <h3>1. Setting Up the Storage Directory</h3>
    <pre><code>
import os
from datetime import datetime

# Define the directory where screenshots will be saved
SAVE_DIR = "screenshots"
os.makedirs(SAVE_DIR, exist_ok=True)
    </code></pre>

    <p><strong>Rationale:</strong> Saving images in a specific folder centralizes all files, making management and future retrieval easier. The <code>exist_ok=True</code> parameter ensures that if the directory already exists, it won‚Äôt be recreated, avoiding errors.</p>

    <h3>2. Defining the Capture Interval</h3>
    <pre><code>
# Set the interval for screenshots in seconds
INTERVAL_SECONDS = 60  # Adjustable as needed
    </code></pre>

    <p><strong>Rationale:</strong> Configurable interval provides flexibility to adjust the screenshot frequency, making the script adaptable to different use cases.</p>

    <h3>3. Screenshot Capture Function</h3>
    <pre><code>
from PIL import ImageGrab

# Function to capture and save the screen image
def capture_screenshot():
    # Generate a filename based on date and time
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = os.path.join(SAVE_DIR, f"screenshot_{timestamp}.png")
    
    # Capture the screen and save the image
    screenshot = ImageGrab.grab()
    screenshot.save(filename, "PNG")
    print(f"Screenshot saved at {filename}")
    </code></pre>

    <p><strong>Rationale:</strong> Timestamp-based filenames provide a unique identifier for each image. PNG format preserves visual quality, beneficial for future processing like OCR.</p>

    <h3>4. Scheduling the Screenshot Capture with <code>schedule</code></h3>
    <pre><code>
import schedule
import time

# Schedule the capture function
schedule.every(INTERVAL_SECONDS).seconds.do(capture_screenshot)
    </code></pre>

    <p><strong>Rationale:</strong> This code line schedules the <code>capture_screenshot()</code> function to run at the specified interval, providing an intuitive and simple scheduling solution.</p>

    <h3>5. Continuous Loop to Keep the Program Running</h3>
    <pre><code>
# Run the loop to keep the capture active
print("Starting automatic screenshot capture...")
while True:
    schedule.run_pending()
    time.sleep(1)
    </code></pre>

    <p><strong>Rationale:</strong> The <code>while True</code> loop keeps the script running, and <code>schedule.run_pending()</code> checks if it‚Äôs time to execute the function, with <code>time.sleep(1)</code> optimizing CPU usage.</p>

    <h2>Testing the Code and Final Considerations</h2>

    <h3>Testing</h3>
    <ul>
        <li><strong>Directory Verification</strong>: Upon first running the script, confirm the directory <code>screenshots</code> is created with timestamped files.</li>
        <li><strong>Interval Testing</strong>: Test with different values for <code>INTERVAL_SECONDS</code> to ensure it performs captures at the defined interval.</li>
    </ul>

    <h3>Future Considerations</h3>
    <p>The code is modular, allowing easy integration of future functionalities such as text recognition, data vectorization, encryption, or database integration.</p>


<hr>
<!-- Project 2: AI Image Processing Pipeline -->
<h1>Project 2: AI Image Processing Pipeline</h1>
<p>This document outlines the second step in our AI-driven image processing project, expanding our automatic screenshot capture with AI capabilities to analyze screenshots, identify open applications, perform OCR to extract text, and generate metadata for storage and retrieval.</p>

<h2>Objective</h2>
<p>The objective is to capture relevant content from the screenshots to enable efficient data retrieval and allow users to search for specific events. The processing pipeline is divided into three tasks:</p>
<ul>
    <li><strong>Application Identification:</strong> Detect and identify visual elements of open applications on the screen.</li>
    <li><strong>Text Recognition (OCR):</strong> Extract visible text, which will be crucial for future searches and providing context.</li>
    <li><strong>Metadata Generation:</strong> Structure and store relevant information about the screenshot, such as timestamp, identified applications, and extracted text.</li>
</ul>

<h2>Selected Tools and Libraries</h2>
<h3>1. OpenCV</h3>
<p><strong>Reason:</strong> OpenCV is a powerful and well-documented computer vision library with a wide range of image manipulation functions.</p>
<p><strong>Usage in the Project:</strong> We‚Äôll use OpenCV for image pre-processing, including:</p>
<ul>
    <li>Converting images to grayscale, which improves OCR accuracy.</li>
    <li>Edge detection and contouring, which will help segment and analyze visual elements to identify areas of interest.</li>
</ul>

<h3>2. Tesseract OCR and Pytesseract</h3>
<p><strong>Reason:</strong> Tesseract is one of the most reliable tools for text recognition, especially in scenarios where text is mixed with graphics, like screenshots.</p>
<p><strong>Usage in the Project:</strong> We‚Äôll use Tesseract to extract all visible text from the image, allowing indexing and retrieval of textual content.</p>

<h2>Detailed Step-by-Step Pipeline</h2>

<h3>1. Image Preprocessing</h3>
<pre><code>
import cv2
from PIL import Image
import pytesseract

def preprocess_image(image):
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
_, binary_image = cv2.threshold(gray_image, 150, 255, cv2.THRESH_BINARY)
return binary_image
</code></pre>

<p><strong>Technical Justification:</strong> Converting to grayscale removes unnecessary color information, which reduces noise and increases OCR efficiency. Thresholding simplifies the image for OCR, making text more defined and eliminating non-text background elements.</p>

<h3>2. Text Recognition with OCR</h3>
<pre><code>
def extract_text(preprocessed_image):
extracted_text = pytesseract.image_to_string(preprocessed_image, lang='eng')
return extracted_text
</code></pre>

<p><strong>Technical Justification:</strong> Setting the language to English improves accuracy. The prior thresholding focuses OCR on the text, ignoring icons and background elements.</p>

<h3>3. Application Identification with Contour Detection Techniques</h3>
<pre><code>
def identify_applications(image):
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
edges = cv2.Canny(gray_image, 100, 200)
contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
detected_applications = []
for contour in contours:
    area = cv2.contourArea(contour)
    if area > 10000:
        detected_applications.append("App Detected")
return detected_applications    
</code></pre>

<p><strong>Technical Justification:</strong> Edge detection and contour analysis allow focusing on larger regions that represent windows or dialog boxes, acting as a heuristic method to identify applications.</p>

<h3>4. Structured Metadata Generation</h3>
<pre><code>
from datetime import datetime
import json

def generate_metadata(filename, extracted_text, detected_applications, image_dimensions):
metadata = {
    "filename": filename,
    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "extracted_text": extracted_text,
    "open_applications": detected_applications,
    "image_dimensions": image_dimensions
}
metadata_filename = filename.replace(".png", ".json")
with open(metadata_filename, 'w') as f:
    json.dump(metadata, f, indent=4)
return metadata    
</code></pre>

<p><strong>Technical Justification:</strong> JSON storage allows for easy querying and integration. The metadata includes essential elements for future search and indexing.</p>

<h2>Different Approaches</h2>
Many different approaches can be implemented using an AI model to analyse the content of captured images for text extraction or object recognition. How:

    <p>Example using a pre-trained AI model like OpenAI CLIP, processes the image using an AI model like CLIP to describe content.

    <pre><code>
    def process_image_with_clip(image_path, model):
    image = Image.open(image_path)
    description = model.describe_image(image)  # Hypothetical method
    return description
    </code></pre>

<p>This code loads a screenshot and uses an AI model to extract text, recognize objects, or generate a description. With models like CLIP, you can build a comprehensive understanding of image content, aiding future search queries.</p>

<h2>Project Validation</h2>
<p>Test the OCR and application identification functions with various screenshots to ensure accuracy. Adjust parameters as needed for optimized performance.</p>

<h2>Future Considerations</h2>
<ul>
    <li><strong>Advanced Application Identification:</strong> Integrate deep learning models for more precise detection of application icons and window patterns.</li>
    <li><strong>Integration with Vector Databases:</strong> Use vector databases for semantic search capabilities, enhancing data retrieval by contextual similarity.</li>
    <li><strong>Enhanced Security and Data Privacy:</strong> Encrypt screenshots and metadata to handle sensitive information securely.</li>
    <li><strong>User Interface for Metadata Management:</strong> Develop a UI for visualizing and managing metadata with advanced query options.</li>
    <li><strong>Multi-Language OCR Support:</strong> Expand OCR capabilities to recognize multiple languages for global applicability.</li>
</ul>
<p>This pipeline provides a robust and extensible base for real-time information retrieval, productivity optimization, and data security for companies, positioning the tool for future enhancements.</p>



<hr>
<!-- Project 3: Vectorized Database Storage -->
<h1>Project 3: Vectorized Database Storage</h1>

    <h2>Project Overview and Rationale</h2>
    <h3>Objective</h3>
    <p>
        Our objective is to implement a vectorized database system to store and retrieve data extracted from screenshots, 
        including processed text and metadata. The vectorized storage will allow advanced semantic searches by transforming 
        the extracted text from each screenshot into vector representations that capture the context and meaning of the content.
    </p>

    <h3>Technical Decisions</h3>
    <p>
        To achieve this goal, we‚Äôll use the following tools and techniques:
    </p>
    <ul>
        <li><strong>FAISS (Facebook AI Similarity Search):</strong> FAISS is a highly optimized vector search engine developed by Meta (Facebook).
            It‚Äôs efficient for searching large datasets of vectors, allowing fast retrieval based on vector similarity‚Äîa key requirement for semantic search.
        </li>
        <li><strong>Transformers and Embeddings Models:</strong> We‚Äôll use a language embedding model (such as <code>BERT</code> or <code>SBERT</code>)
            to convert extracted text into high-dimensional vector representations. This embedding process captures the contextual meaning of the textual content.
        </li>
    </ul>

    <h2>Step-by-Step Guide</h2>
    
    <h3>1. Environment Preparation</h3>
    <h4>1.1. Install Required Libraries</h4>
    <pre><code>pip install faiss-cpu transformers torch</code></pre>
    <ul>
        <li><strong>FAISS:</strong> For vectorized storage and fast similarity searches.</li>
        <li><strong>Transformers and Torch:</strong> We‚Äôll use the <code>transformers</code> library to load and process embeddings with advanced language models like BERT.</li>
    </ul>

    <h3>2. Setting Up the Text Embedding Model</h3>
    <p>We‚Äôll use the <code>distilbert-base-nli-stsb-mean-tokens</code> model for semantic embedding efficiency.</p>
    <pre><code>from transformers import AutoModel, AutoTokenizer
import torch

# Load the embedding model
embedding_model = "sentence-transformers/distilbert-base-nli-stsb-mean-tokens"
tokenizer = AutoTokenizer.from_pretrained(embedding_model)
model = AutoModel.from_pretrained(embedding_model)

# Function to generate embeddings
def generate_embedding(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1)
    return embedding.squeeze().numpy()</code></pre>

    <h3>3. Setting Up the FAISS Vectorized Database</h3>
    <p>Now, we configure a FAISS vector index, using an <code>IVFFlat</code> index.</p>
    <pre><code>import faiss
import numpy as np

# Define the embedding vector dimension
dimension = 768

# Create the FAISS index
quantizer = faiss.IndexFlatL2(dimension)
faiss_index = faiss.IndexIVFFlat(quantizer, dimension, 100)

# Train the FAISS index
def train_index(vectors):
    faiss_index.train(np.array(vectors, dtype=np.float32))
    print("FAISS index trained successfully.")

# Function to add vectors to the index
def add_vector_to_index(id, vector):
    vector_np = np.array([vector], dtype=np.float32)
    faiss_index.add_with_ids(vector_np, np.array([id], dtype=np.int64))</code></pre>

    <h3>4. Inserting and Storing Data in FAISS</h3>
    <p>Integrating processed screenshots and metadata by generating embeddings and adding to FAISS with a unique ID.</p>
    <pre><code>import os
import json

# Function to load and store metadata in the FAISS index
def store_data_in_index(metadata_directory):
    vectors = []
    ids = []
    for file in os.listdir(metadata_directory):
        if file.endswith(".json"):
            filepath = os.path.join(metadata_directory, file)
            with open(filepath, 'r') as f:
                metadata = json.load(f)
                text = metadata["extracted_text"]
                id = int(metadata["timestamp"].replace("-", "").replace(" ", "").replace(":", ""))
                embedding = generate_embedding(text)
                add_vector_to_index(id, embedding)
                vectors.append(embedding)
                ids.append(id)
    train_index(vectors)</code></pre>

    <h3>5. Semantic Search in the FAISS Index</h3>
    <p>This function allows users to input a query and get contextually similar results.</p>
    <pre><code># Function to perform semantic search in the FAISS index
def search_index(query, top_k=5):
    query_embedding = generate_embedding(query)
    query_vector = np.array([query_embedding], dtype=np.float32)
    D, I = faiss_index.search(query_vector, top_k)
    results = []
    for dist, id in zip(D[0], I[0]):
        results.append({"id": id, "distance": dist})
    return results</code></pre>

    <h2>Different Approaches</h2>
    <p>Store processed data (descriptions and metadata) in a vectorized database, allowing efficient similarity searches for relevant content.</p>

    <pre><code class="language-python">
    from sentence_transformers import SentenceTransformer
    import faiss
    import numpy as np

    # Initialize the embedding model and FAISS index
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    index = faiss.IndexFlatL2(384)  # 384 dimensions for MiniLM embeddings
    metadata_storage = {}  # Dictionary to map index IDs to descriptions or paths

def store_data(description, image_path):
    """Encodes the description to a vector, adds it to the FAISS index, and stores metadata."""
    vector = model.encode([description])
    index.add(np.array(vector, dtype='float32'))
    metadata_storage[len(metadata_storage)] = {'description': description, 'path': image_path}

def search_content(search_query):
    """Searches for similar descriptions using FAISS and returns metadata for the closest matches."""
    query_vector = model.encode([search_query])
    D, I = index.search(np.array(query_vector, dtype='float32'), k=5)  # Retrieve top 5 results
    results = [metadata_storage[idx] for idx in I[0]]
    return results
</code></pre>

<p>This setup stores descriptions as vectors and enables fast similarity search via FAISS. The function `search_content` uses a vectorized search query to locate the most relevant stored descriptions and their paths, ideal for managing and searching through large datasets.</p>

    <h2>Testing and Validating the Code</h2>
    <ul>
        <li><strong>Indexing Tests:</strong> Test indexing with a small dataset to evaluate performance.</li>
        <li><strong>Semantic Search Testing:</strong> Conduct searches with terms related to the screenshots to validate search accuracy.</li>
    </ul>

    <h2>Final Considerations</h2>
    <p>
        This solution provides a complete vectorized storage and semantic searching setup for screenshots and metadata. 
        With FAISS as the backend, the system is highly scalable and ready for efficient operations.
    </p>




<hr>    
<!-- Project 4: Search for Past Events -->
<h1>Project 4: Search for Past Events</h1>
<p>This is a step-by-step guide for the fourth stage, ‚ÄúSearch for Past Events,‚Äù with a highly professional and robust approach. This guide builds upon the previous stages and sets up an advanced search system for retrieving historical data and events based on semantic relevance and user-defined filters.</p>

<h2>Project Structure and Rationale</h2>
<h3>Objective</h3>
<p>To build a semantic and filterable search system that retrieves past events from stored screenshot data and metadata. This system will support two primary types of queries:</p>
<ol>
    <li><strong>Semantic relevance search</strong>: Retrieve past events based on textual input, using the vectorized database to identify contextual similarity.</li>
    <li><strong>Filtered search</strong>: Allow searches based on specific filters like date, application, or keywords.</li>
</ol>

<h3>Technical Decisions</h3>
<p>To implement the search functionality, we‚Äôll use:</p>
<ul>
    <li><strong>FAISS</strong>: For efficient vector similarity searches.</li>
    <li><strong>Pandas</strong>: For effective management and filtering of metadata.</li>
    <li><strong>Modular Query Interface</strong>: A modular design that can be integrated into various front-end interfaces, such as APIs, command-line interfaces, or graphical user interfaces.</li>
</ul>

<h2>Detailed Code Structure and Rationale</h2>
<p>This project will consist of five primary steps, integrated to form a robust, flexible search system that allows for future expansion.</p>

<h2>Step-by-Step Project</h2>

<h3>1. Environment Preparation and Data Loading</h3>
<h4>1.1. Importing Required Libraries</h4>
<p>Install the <code>pandas</code> library to manage metadata efficiently and facilitate query filtering.</p>
<pre><code>pip install pandas</code></pre>

<h4>1.2. Loading Metadata into Pandas</h4>
<pre><code>
import os
import pandas as pd
import json

# Directory containing JSON metadata files
metadata_directory = "metadata"

# Function to load metadata into a DataFrame
def load_metadata(directory):
    records = []
    
    for file in os.listdir(directory):
        if file.endswith(".json"):
            path = os.path.join(directory, file)
            with open(path, 'r') as f:
                metadata = json.load(f)
                records.append(metadata)
    
    # Converting the list of records into a DataFrame
    df_metadata = pd.DataFrame(records)
    return df_metadata

# Load all metadata into the DataFrame
df_metadata = load_metadata(metadata_directory)
</code></pre>

<h3>2. Semantic Search Function with FAISS</h3>
<pre><code>
def semantic_search(query, top_k=5):
    # Generate embedding for the query
    query_embedding = generate_embedding(query)
    query_vector = np.array([query_embedding], dtype=np.float32)
    
    # Search in the FAISS index
    D, I = faiss_index.search(query_vector, top_k)
    
    # Retrieve results based on IDs and distances
    results = []
    for dist, id in zip(D[0], I[0]):
        if id != -1:  # Ensure ID is valid
            result = df_metadata[df_metadata["timestamp"] == str(id)]
            if not result.empty:
                result = result.iloc[0].to_dict()
                result["similarity"] = dist
                results.append(result)
    
    # Sort by similarity
    results = sorted(results, key=lambda x: x["similarity"])
    return results
</code></pre>

<h3>3. Filtered Search Function</h3>
<pre><code>
from datetime import datetime

# Filtered search function
def filtered_search(app=None, start_date=None, end_date=None):
    # Apply filters to the metadata
    df_result = df_metadata.copy()
    
    # Filter by open application
    if app:
        df_result = df_result[df_result["open_applications"].apply(lambda x: app in x)]
    
    # Filter by date range
    if start_date:
        start_date = datetime.strptime(start_date, "%Y-%m-%d")
        df_result = df_result[pd.to_datetime(df_result["timestamp"]) >= start_date]
    
    if end_date:
        end_date = datetime.strptime(end_date, "%Y-%m-%d")
        df_result = df_result[pd.to_datetime(df_result["timestamp"]) <= end_date]
    
    return df_result.to_dict(orient="records")
</code></pre>

<h3>4. Complete Search Function (Semantic + Filters)</h3>
<pre><code>
def comprehensive_search(query=None, app=None, start_date=None, end_date=None, top_k=5):
    # First, perform a semantic search if there is a query
    if query:
        semantic_results = semantic_search(query, top_k=top_k)
        df_result = pd.DataFrame(semantic_results)
    else:
        df_result = df_metadata.copy()
    
    # Apply additional filters
    if app:
        df_result = df_result[df_result["open_applications"].apply(lambda x: app in x)]
    
    if start_date:
        start_date = datetime.strptime(start_date, "%Y-%m-%d")
        df_result = df_result[pd.to_datetime(df_result["timestamp"]) >= start_date]
    
    if end_date:
        end_date = datetime.strptime(end_date, "%Y-%m-%d")
        df_result = df_result[pd.to_datetime(df_result["timestamp"]) <= end_date]
    
    # Convert results into a list of dictionaries
    return df_result.to_dict(orient="records")
</code></pre>

<h3>5. Search Interface</h3>
<pre><code>
def search_interface():
    query = input("Enter your semantic search query (or press Enter to skip): ")
    app = input("Filter by application (or press Enter to skip): ")
    start_date = input("Start date (YYYY-MM-DD) (or press Enter to skip): ")
    end_date = input("End date (YYYY-MM-DD) (or press Enter to skip): ")
    
    # Perform a comprehensive search with provided parameters
    results = comprehensive_search(
        query=query if query else None,
        app=app if app else None,
        start_date=start_date if start_date else None,
        end_date=end_date if end_date else None
    )
    
    # Display the results
    for result in results:
        print(f"\nFound result:\n{result}\n")
</code></pre>

<p>Implement a simple user interface for searching through stored metadata and retrieving relevant results.</p>

<pre><code class="language-python">
def search_interface():
    """Provides a text-based interface to search for stored metadata."""
    query = input("Enter what you want to find: ")
    results = search_content(query)
    if results:
        print("Found relevant entries:")
        for result in results:
            print(f"Description: {result['description']}, Image Path: {result['path']}")
    else:
        print("No relevant results found.")

# Alternative: Implement a web-based interface with Streamlit
import streamlit as st
from PIL import Image

def streamlit_search_interface():
    """Creates a search interface with Streamlit for easier access and display of images."""
    query = st.text_input("What are you looking for?")
    if query:
        results = search_content(query)
        if results:
            for result in results:
                st.write(f"Description: {result['description']}")
                st.image(Image.open(result['path']))
        else:
            st.write("No relevant results found.")
</code></pre>
<p>The `search_interface` function provides a simple CLI for users to search stored data. Alternatively, `streamlit_search_interface` offers a GUI, showing image descriptions and displaying the images directly, enhancing the search experience.</p>

<h2>Different Approaches</h2>
<p>Implement a simple user interface for searching through stored metadata and retrieving relevant results. Basic.</p>

<pre><code class="language-python">
def search_interface():
    """Provides a text-based interface to search for stored metadata."""
    query = input("Enter what you want to find: ")
    results = search_content(query)
    if results:
        print("Found relevant entries:")
        for result in results:
            print(f"Description: {result['description']}, Image Path: {result['path']}")
    else:
        print("No relevant results found.")

# Alternative: Implement a web-based interface with Streamlit
import streamlit as st
from PIL import Image

def streamlit_search_interface():
    """Creates a search interface with Streamlit for easier access and display of images."""
    query = st.text_input("What are you looking for?")
    if query:
        results = search_content(query)
        if results:
            for result in results:
                st.write(f"Description: {result['description']}")
                st.image(Image.open(result['path']))
        else:
            st.write("No relevant results found.")
</code></pre>
<p>The `search_interface` function provides a simple CLI for users to search stored data. Alternatively, `streamlit_search_interface` offers a GUI, showing image descriptions and displaying the images directly, enhancing the search experience.</p>

<h2>Code Validation and Testing</h2>
<ol>
    <li><strong>Semantic Search Testing</strong>: Run queries with relevant terms and review the results to validate accuracy and ranking.</li>
    <li><strong>Filtered Search Testing</strong>: Execute searches using date and application filters to ensure filters are functioning as expected.</li>
    <li><strong>Performance Validation</strong>: Evaluate performance by running multiple searches simultaneously, ensuring the system can handle concurrent requests efficiently.</li>
</ol>

<h2>Final Considerations</h2>
<p>This advanced search system offers a complete solution for retrieving historical data and events. With the modular structure and use of FAISS for semantic search, the system is scalable and can easily integrate with complex interfaces like APIs. This project is now ready for real-time operations, providing an efficient, precise, and customizable search experience.</p>

<hr>
<!-- Voice Driven Mobile App Project Plan -->
<h1>Project 5: Voice-Driven Mobile App for Semantic and Filtered Search of Past Events</h1>
    
<p>A sophisticated mobile application that enables users to search historical data and events using voice commands. This app integrates advanced NLP for intent recognition, a vectorized back-end search system, and a user-friendly interface with customizable filters.</p>

<p>Voice Input and Transcription: Users interact through voice commands, transcribed using multiple APIs (e.g., Google Speech-to-Text, Deepgram) with fallback options for reliable functionality.</p>

<p>NLP Intent Recognition: Powered by models like GPT-4 and Dialogflow, the app accurately identifies search types, filter criteria, and user intents, enabling precise and contextual searches.</p>

<p>Search Integration with Back-End: Connects to a FAISS-based vectorized database, supporting semantic and filterable searches by app, date, and event relevance for fast and accurate results.</p>

<p>Customizable Search Display: An interactive results screen lets users refine results by date, app, or relevance, with expandable cards for detailed information on each event.</p>

<p>Data Privacy and Compliance: Ensures compliance with GDPR/CCPA, using secure data encryption, anonymization, and customizable retention policies to protect user privacy.</p>

    <h2>Project Structure and Rationale</h2>

    <h3>Core Objectives</h3>
    <ol>
        <li>Capture Voice Input with Multi-API Support: Provide reliable voice input handling with dynamic fallback between multiple APIs for high availability.</li>
        <li>Advanced Intent Recognition with NLP: Classify user queries accurately with state-of-the-art NLP models and rule-based fallback logic.</li>
        <li>Seamless API Integration for Fast, Relevant Searches: Connect to a FAISS-based back-end search system for robust, scalable data retrieval.</li>
        <li>User-Centric and Adaptive UI/UX: Offer a responsive, intuitive, and accessible interface with support for multiple languages, themes, and interactive search results.</li>
        <li>Compliance with Data Privacy Regulations: Ensure all data handling is compliant with GDPR, CCPA, and other privacy regulations.</li>
    </ol>

    <h2>Detailed Project Development</h2>
    <p>This high-level project is divided into several phases, each building upon the previous stage and focusing on scalability, flexibility, and professional best practices.</p>

    <h3>1. UI/UX Design in Bubble for an Intuitive, Adaptive Interface</h3>
    
    <h4>1.1. Home Screen Design</h4>
    <p>Elements:</p>
    <ul>
        <li>Microphone Button: Users tap to activate voice input.</li>
        <li>Loading Indicator: Display while transcribing voice input or performing back-end search operations.</li>
        <li>Transcription Display: A text box showing the transcribed query for user confirmation.</li>
    </ul>
    <p>Interactivity:</p>
    <ul>
        <li>Microinteractions: Use subtle animations for all actions, e.g., a ‚Äúlistening‚Äù indicator on the microphone icon.</li>
        <li>Onboarding Tutorial: Provide a quick tutorial for new users, covering voice input and search options.</li>
        <li>Accessibility Features: Large icons, clear text, and high contrast to enhance usability for visually impaired users.</li>
    </ul>

    <h4>1.2. Search Results Screen Design</h4>
    <p>Elements:</p>
    <ul>
        <li>Repeating Group: Dynamic display of search results, including event name, timestamp, similarity score, and relevant metadata.</li>
        <li>Filter Controls: Allow filtering by relevance, date, or app.</li>
        <li>Expandable Result Cards: Users can tap to expand each card for more details.</li>
    </ul>

    <h4>1.3. Settings Screen Design</h4>
    <p>Elements:</p>
    <ul>
        <li>Language Selector: Support multi-language options.</li>
        <li>Voice API Preference: Users choose between Google Speech-to-Text, Azure, or Deepgram.</li>
        <li>Help Section: Include FAQs and voice command tips.</li>
    </ul>

    <h3>2. Robust Voice Recognition and Transcription</h3>
    
    <h4>2.1. Voice Recognition with Multiple APIs</h4>
    <p>Primary API: Google Speech-to-Text for accuracy and extensive language support.</p>
    <p>Fallback APIs: Azure Speech Services, Deepgram API, Whisper (OpenAI)</p>

    <h4>2.2. Audio Preprocessing</h4>
    <p>Noise Reduction and Gain Control: Apply preprocessing on the server-side for clearer transcription.</p>

    <h4>2.3. Error Handling and Fallback Logic</h4>
    <p>Graceful Fallback: Implement a fallback sequence among APIs if the primary API fails.</p>

    <h3>3. Advanced NLP for Intent Recognition</h3>
    
    <h4>3.1. Integrate State-of-the-Art NLP Models</h4>
    <p>Primary Model: GPT-4 for high accuracy in intent recognition and parsing.</p>

    <h4>3.2. Enhancing Intent Recognition with Named Entity Recognition (NER)</h4>
    
    <h4>3.3. Fallback Rule-Based Classification in Bubble</h4>
    
    <h3>4. Scalable API Integration with the FAISS-Based Search System</h3>
    
    <h4>4.1. Design a Modular API System</h4>
    
    <h4>4.2. Asynchronous Processing for Scalability</h4>
    
    <h4>4.3. Implement Rate Limiting and Monitoring</h4>

    <h3>5. Search Results Display and Enhanced User Experience</h3>
    
    <h4>5.1. Interactive and Customizable Search Results</h4>
    
    <h4>5.2. Personalization and Recommendation-Based Search</h4>
    
    <h4>5.3. Export and Data Visualization</h4>

    <h3>6. Data Privacy and Security Compliance</h3>
    
    <h4>6.1. Implement Data Privacy Policies</h4>
    
    <h4>6.2. Anonymization and Data Retention Settings</h4>
    
    <h4>6.3. Encryption and Secure Data Transmission</h4>

    <h3>7. Scalability and Future Integrations</h3>
    
    <h4>7.1. Microservices Architecture for High Scalability</h4>
    
    <h4>7.2. Predictive Analytics for Advanced Personalization</h4>
    
    <h4>7.3. Future Compatibility with Augmented Reality (AR) and Virtual Assistants</h4>

    <h2>Final Considerations</h2>
    <p>This comprehensive and advanced project plan ensures the mobile app is scalable, secure, and highly adaptable. The integration of top-tier AI models, robust UI/UX design, and multi-layered privacy compliance makes this application suitable for both enterprise environments and high-demand markets. With an emphasis on user experience, security, and advanced technology, this project exemplifies a modern, professional-grade mobile solution for information retrieval.</p>
    




    <!-- Advanced Technical Data -->
    <h2 id="advanced-technical-data">üîç Advanced Technical Data</h2>
    <ul>
        <li><strong>Screenshot Capture:</strong> Implemented with the <strong>PyAutoGUI</strong> library, configurable for regular intervals.</li>
        <li><strong>Image Processing:</strong> The AI processes the captures using models such as <strong>OpenAI CLIP</strong> or equivalents.</li>
        <li><strong>Data Storage:</strong> Descriptions are saved in relational or NoSQL databases like <strong>SQLite</strong>, <strong>MongoDB</strong>, or <strong>PostgreSQL</strong>.</li>
        <li><strong>Vector Search:</strong> Information is vectorized in a <strong>vector store</strong>, allowing fast and efficient searches through embeddings using <strong>FAISS</strong>.</li>
        <li><strong>Python:</strong> Main programming language of the project.</li>
    </ul>

    <!-- Opportunities for Improvement and Commercial Expansion -->
    <h2 id="opportunities-for-improvement-and-commercial-expansion">üìà Opportunities for Improvement and Commercial Expansion</h2>

    <!-- Potential Improvements -->
    <h3 id="potential-improvements">Potential Improvements</h3>
    <ol>
        <li><strong>Image Recognition Enhancement:</strong> Develop AI models specifically trained for the company‚Äôs context, increasing search accuracy and relevance.</li>
        <li><strong>Advanced Semantic Search:</strong> Implement a search system based on user intent, going beyond simple keywords with advanced LLM.</li>
        <li><strong>Integration with Corporate Tools:</strong> Add support for platforms like Microsoft 365, Slack, Trello, among others.</li>
        <li><strong>Security and Compliance:</strong> Ensure robust encryption and adherence to regulations such as GDPR.</li>
    </ol>

    <!-- Commercial Possibilities -->
    <h3 id="commercial-possibilities">Commercial Possibilities</h3>
    <ul>
        <li><strong>Software Licensing:</strong> Market as licensed software for companies or offer as a SaaS.</li>
        <li><strong>Consulting and Customization:</strong> Offer consulting services to adapt the solution to specific client needs.</li>
        <li><strong>Niche Exploration:</strong> Adapt for productivity, legal sector, or creative agencies.</li>
        <li><strong>Premium Services:</strong> Offer premium versions with advanced features.</li>
    </ul>

    <!-- Legal Disclaimer -->
    <h2 id="legal-disclaimer">‚ö†Ô∏è Legal Disclaimer</h2>
    <blockquote>
        <p style="text-align: justify;">This project, called ‚ÄúRemember: A Link to the Past‚Äù, is NOT a clone, copy, or direct reproduction of any proprietary technology. This project was developed independently and is intended to demonstrate concepts of screen capture, image processing with artificial intelligence, and vectorized information search systems, using open-source tools and libraries.</p>
    </blockquote>

    <!-- Use of Terms and Comparisons -->
    <h2 id="use-of-terms-and-comparisons">üìú Use of Terms and Comparisons</h2>
    <blockquote>
        <p>Does not use proprietary codes, algorithms, or methods.</p>
        <p>Does not appropriate** any resources, libraries, or APIs improperly or without compliance with intellectual property laws.</p>
        <p>Does not infringe patents, copyrights, or trademarks of any entity.</p>
    </blockquote>

    <!-- Project Nature -->
    <h2 id="project-nature">üåç Project Nature</h2>
    <blockquote>
        <p>This project was developed with educational and experimental purposes, aimed at exploring and demonstrating widely known techniques in the area of screen capture and artificial intelligence, although it may lead to later commercial applications. All code provided here is public domain knowledge or built using open-source tools, constructed from scratch without the use of any proprietary components.</p>
    </blockquote>

    <!-- Intellectual Property -->
    <h2 id="respect-for-intellectual-property-laws">üìú Respect for Intellectual Property Laws</h2>
    <blockquote>
        <p>The author of the project strictly respects intellectual property laws and encourages responsible and ethical use of the code. The commercial use or distribution of this project is not prohibited but must comply with all applicable intellectual property protection regulations.</p>
    </blockquote>

</div>
<hr>
<!-- Project Section: End -->




<!-- Sponsor -->
<div class="iframe-container">
    <iframe id="dynamic-iframe" width="100%" height="200" src="https://github.com/sponsors/MuriloKrominski/card" title="Sponsor MuriloKrominski"></iframe>
    </div>
    <iframe src="https://github.com/sponsors/MuriloKrominski/button" title="Sponsor MuriloKrominski" height="32" width=100% style="border: 0; border-radius: 6px;"></iframe>
<!-- Footer Section -->
    <footer>
    <p><a href="https://github.com/MuriloKrominski">&copy; 2024 Murilo Krominski. All rights reserved.</a></p>
    <p><a href="https://github.com/MuriloKrominski">Created with ‚ù§Ô∏è and maintained by Murilo Krominski.</a></p>
    </footer>
    
<!-- Custom JavaScript file for additional functionalities -->
    <script src="https://murilokrominski.github.io/scripts.js"></script>
<!-- Highlight.js library for syntax highlighting of code blocks -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<!-- Initialize Highlight.js for all code blocks -->
    <script>hljs.highlightAll();</script> 
    <!-- External Stylesheets -->
    <link rel="stylesheet" href="style.css">
<!-- Google Translate Element (invisible) -->
    <div id="google_translate_element" style="visibility: hidden">
    </div>
</body>
</html>